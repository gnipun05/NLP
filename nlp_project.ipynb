{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Operating System Textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading book and converting it into text variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"OS_book.txt\",encoding='utf-8')\n",
    "wordslist = file.read().splitlines() # to escape \\n occurence\n",
    "wordslist = [i for i in wordslist if i!='']\n",
    "text = \"\"\n",
    "text = text.join(wordslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a string which has all the punctuations to be removed\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./‘’?“”@#$%^&*_~'''\n",
    "cleantext = \"\"\n",
    "for char in text:\n",
    "    if char not in punctuations:\n",
    "        cleantext = cleantext + char\n",
    "        \n",
    "#Converting the text into lower case   \n",
    "cleantext = cleantext.lower()\n",
    "text = cleantext.replace(\"figure\",'')\n",
    "cleantext = re.sub(r'[cC]hapter[0-9]+','',cleantext)\n",
    "cleantext = re.sub(r'[0-9]+','',cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud without removing stopwords\n",
    "wordcloud = WordCloud(width = 800, height = 600, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10,stopwords = {},colormap='winter').generate(cleantext) \n",
    "\n",
    "plt.figure(figsize = (12,8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(cleantext)\n",
    "freq = nltk.FreqDist(tokens)\n",
    "plt.figure(figsize=(12,5))\n",
    "freq.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(cleantext)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords and storing it into finaltext\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(cleantext)\n",
    "tokens_final = [i for i in tokens if not i in stop_words] # tokenising with removing stopwords\n",
    "finaltext = \"  \"\n",
    "finaltext = finaltext.join(tokens_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud after removing stopwords\n",
    "wordcloud = WordCloud(width = 800, height = 600, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10,stopwords = {},colormap='winter').generate(finaltext) \n",
    "\n",
    "plt.figure(figsize = (12,8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distribution of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(finaltext)\n",
    "tokens = [i for i in tokens if not i in stop_words]\n",
    "freq = nltk.FreqDist(tokens)\n",
    "plt.figure(figsize=(12,5))\n",
    "freq.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens) \n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter( tag for word,  tag in tagged)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tags = nltk.FreqDist(counts)\n",
    "plt.figure(figsize=(12,5))\n",
    "freq_tags.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For word length vs Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bin_size=np.linspace(0,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Wordlength and storing it as a list\n",
    "wordLength = [len(r) for r in tokens]\n",
    "\n",
    "#Plotting histogram of Word length vs Frequency\n",
    "plt.hist(wordLength, bins=bin_size)\n",
    "plt.xlabel('word length')\n",
    "plt.ylabel('word length Frequency')\n",
    "plt.title('Frequency Distribution for the book SHERLOCK')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a26695c0aa70e9aa3ba869a6bd0927c8cada8633cf86cda87e8f1e7197e1867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
